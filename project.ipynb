{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import csv\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import json\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "import spacy\n",
    "import requests\n",
    "response = requests.get(\"https://translate.google.com/\")\n",
    "import gensim.corpora as corpora\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(total=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"https://\", adapter)\n",
    "response = session.get(\"https://translate.google.com\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Giving your own tweets(This might be a solution to the tweet limit in twitter).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = []\n",
    "ID.append(str(\"@\") + input(\"Your Twitter ID: \"))\n",
    "Password = input(\"Your Twitter Password: \")\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "tweets = []\n",
    "times = []\n",
    "UserTags = []\n",
    "number = int(input(\"Number of Tweets you wish to apply with (50 < Suggsted): \"))\n",
    "for i in ID:\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://twitter.com/i/flow/login\")\n",
    "    time.sleep(5)\n",
    "    giris = browser.find_element(By.NAME,\"text\")\n",
    "    giris.send_keys(ID)\n",
    "    giris.send_keys(Keys.RETURN)\n",
    "    time.sleep(1.2)\n",
    "    browser.find_element(By.NAME,\"password\").send_keys(Password)\n",
    "    browser.find_element(By.NAME,\"password\").send_keys(Keys.RETURN)\n",
    "    time.sleep(3)\n",
    "    browser.find_element(By.XPATH,\"//*[@id='react-root']/div/div/div[2]/main/div/div/div/div[2]/div/div[2]/div/div/div/div[1]/div/div/div/form/div[1]/div/div/div/label/div[2]/div/input\").send_keys(i)\n",
    "    browser.find_element(By.XPATH,\"//*[@id='react-root']/div/div/div[2]/main/div/div/div/div[2]/div/div[2]/div/div/div/div[1]/div/div/div/form/div[1]/div/div/div/label/div[2]/div/input\").send_keys(Keys.RETURN)\n",
    "    time.sleep(3)\n",
    "    browser.find_element(By.XPATH,\"//*[@id='react-root']/div/div/div[2]/main/div/div/div/div/div/div[1]/div[1]/div[2]/nav/div/div[2]/div/div[3]/a\").click()\n",
    "    time.sleep(4)\n",
    "    browser.find_element(By.XPATH,\"//*[@id='react-root']/div/div/div[2]/main/div/div/div/div/div/div[3]/section/div/div/div/div/div[1]/div/div/div/div/div[2]/div[1]/div[1]/div/div[1]/a/div/div[1]/span/span[1]\").click()\n",
    "    articles = browser.find_elements(By.XPATH,\"//article[@data-testid='tweet']\")\n",
    "    time.sleep(0.5)\n",
    "    maxtweet = browser.find_element(By.XPATH,\"//*[@id='react-root']/div/div/div[2]/main/div/div/div/div/div/div[1]/div[1]/div/div/div/div/div/div[2]/div/div\").text\n",
    "    maxtweet = maxtweet[:-7]\n",
    "    if maxtweet[-1] == \"K\":\n",
    "        maxtweet = number + 100\n",
    "    if len(str(maxtweet)) > 4:\n",
    "        if maxtweet[-4] == \",\":\n",
    "            maxtweet = number +100\n",
    "    else:\n",
    "        maxtweet = int(maxtweet)\n",
    "    tweets12 = []\n",
    "    time.sleep(4)\n",
    "    browser.find_element(By.TAG_NAME, value=\"body\").send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.5)\n",
    "    browser.find_element(By.TAG_NAME, value=\"body\").send_keys(Keys.PAGE_DOWN)\n",
    "    browser.find_element(By.TAG_NAME, value=\"body\").send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.5)\n",
    "    browser.find_element(By.TAG_NAME, value=\"body\").send_keys(Keys.PAGE_DOWN)\n",
    "    browser.find_element(By.TAG_NAME, value=\"body\").send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(3)\n",
    "    while True:\n",
    "        for article in articles:\n",
    "            UserTag = browser.find_element(By.XPATH,\".//div[@data-testid='User-Name']\").text\n",
    "            UserTags.append(UserTag)\n",
    "            \n",
    "            Tweet = browser.find_element(By.XPATH,\".//div[@data-testid='tweetText']\").text\n",
    "            tweets.append(Tweet)\n",
    "            tweets12.append(Tweet)\n",
    "            \n",
    "            Timestamp = browser.find_element(By.XPATH,\".//time\").get_attribute(\"datetime\")\n",
    "            times.append(Timestamp)\n",
    "        browser.find_element(By.TAG_NAME, value=\"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(0.6)\n",
    "        articles = browser.find_elements(By.XPATH,\"//article[@data-testid='tweet']\")\n",
    "        tweets2 = list(set(tweets12))\n",
    "        if len(tweets2) > number-1:\n",
    "            break\n",
    "        if maxtweet < number:\n",
    "            if len(tweets2) > maxtweet/2 - 10:\n",
    "                break\n",
    "    browser.close()\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "for timee in range(0,len(times)):\n",
    "    times[timee] = times[timee][0:10]\n",
    "df = pd.DataFrame(zip(UserTags,times,tweets)\n",
    "                  ,columns=['UserTags','TimeStamps','Tweets'])\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "for dd in ID:\n",
    "    df.iloc[number*ID.index(dd):(number*(ID.index(dd)+1)),0] = dd\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "    for timee in range(0,len(times)):\n",
    "        times[timee] = times[timee][0:10]\n",
    "df = pd.DataFrame(zip(UserTags,times,tweets)\n",
    "                  ,columns=['UserTags','TimeStamps','Tweets'])\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "for dd in ID:\n",
    "    df.iloc[number*ID.index(dd):(number*(ID.index(dd)+1)),0] = dd\n",
    "with pd.ExcelWriter(\"project_tweet.xlsx\") as writer:\n",
    "    df.to_excel(writer, sheet_name=\"tweetler\")\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "tweetler = pd.read_excel(\"project_tweet.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Preprocessing and LDA Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(\"https://translate.google.com/\")\n",
    "import gensim.corpora as corpora\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(total=5, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"https://\", adapter)\n",
    "response = session.get(\"https://translate.google.com\")\n",
    "########################################################\n",
    "#Json function for LDA dict\n",
    "file_path = 'LDA_dict.json'\n",
    "def download_dictionary_to_pc(dictionary, file_path):\n",
    "    json_string = json.dumps(dictionary)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(json_string)\n",
    "with open(file_path, 'r') as file:\n",
    "    LDA_dict = json.load(file)\n",
    "tweetler = pd.read_excel(\"project_tweet.xlsx\")\n",
    "#Tag and Mention cleaning.\n",
    "def remove_mentions_and_tags(text):\n",
    "    text = re.sub(r\"@\\S*\", \"\", text)\n",
    "    text = re.sub(r\"\\n\",\"\",text)\n",
    "    return re.sub(r\"#\\S*\", \"\", text)\n",
    "tweetler[\"Tweets\"] = tweetler[\"Tweets\"].apply(remove_mentions_and_tags)\n",
    "print(\"Tags and mentions have been deleted...\")\n",
    "#Only alphabet.(TR dahil)\n",
    "def keep_only_alphabet(text):\n",
    "    return re.sub(r\"[^a-zA-ZğüşıöçĞÜŞİÖÇı]\", \" \", text)\n",
    "tweetler.Tweets = tweetler.Tweets.apply(keep_only_alphabet)\n",
    "tweetler[\"Tweets\"] = tweetler[\"Tweets\"].apply(keep_only_alphabet)\n",
    "print(\"Only the Alphabet...\")\n",
    "# #Language Detect\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "tweetler[\"Language\"] = tweetler[\"Tweets\"].apply(detect_language)\n",
    "print(\"Languages are detected...\")\n",
    "# Delete languages other than tr or eng\n",
    "tweetler = tweetler[tweetler['Language'].isin(['tr', 'en'])]\n",
    "# Ingilizce çeviri(for TR)\n",
    "tweetler = tweetler.reset_index(drop=True)\n",
    "def translate_to_string(text, target_lang):\n",
    "    translator = GoogleTranslator(source='auto', target=target_lang)\n",
    "    translation = translator.translate(text)\n",
    "    translated_text = str(translation)\n",
    "    return translated_text\n",
    "for i in range(0,len(tweetler)):\n",
    "    if tweetler[\"Language\"][i] == \"tr\":\n",
    "        text = tweetler[\"Tweets\"][i]\n",
    "        target_lang = \"en\"  \n",
    "        translated_text = translate_to_string(text, target_lang)\n",
    "        tweetler[\"Tweets\"][i] = translated_text\n",
    "print(\"Translated to English...\")\n",
    "#Lowercase\n",
    "for i in range(0,len(tweetler)):\n",
    "    tweetler[\"Tweets\"][i] = tweetler[\"Tweets\"][i].lower()\n",
    "print(\"Lowercased\")\n",
    "#Url cleaning\n",
    "def remove_url(text):\n",
    "  return re.sub(r\"https?:\\S*\", \"\", text)\n",
    "tweetler[\"Tweets\"] = tweetler[\"Tweets\"].apply(remove_url)\n",
    "print(\"URL Cleaned...\")\n",
    "# Only English words contained.\n",
    "def clean_non_english_words(text):\n",
    "    english_words = set(words.words())\n",
    "    cleaned_words = [word for word in text.split() if word.lower() in english_words]\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text\n",
    "for i in range(0,len(tweetler)):\n",
    "    tweetler[\"Tweets\"][i] = clean_non_english_words(tweetler[\"Tweets\"][i])\n",
    "# Lemmatization\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text\n",
    "for i in range(0,len(tweetler)):\n",
    "    text = tweetler[\"Tweets\"][i]\n",
    "    tweetler[\"Tweets\"][i] = lemmatize_text(text)\n",
    "#Delete Less Than 3 Character words\n",
    "def remove_short_words(text):\n",
    "    pattern = r'\\b\\w{1,2}\\b'\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "for i in range(0,len(tweetler)):\n",
    "    tweetler[\"Tweets\"][i] = remove_short_words(tweetler[\"Tweets\"][i])\n",
    "#Tokenize and clean stopwords\n",
    "tweetler = tweetler[tweetler['Tweets'].str.strip().astype(bool)]\n",
    "tweetler = tweetler.reset_index(drop=True)\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "def remove_stopwords(data_words):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    docs = [nlp(' '.join(map(str, words))) for words in data_words]\n",
    "    filtered_words = [[token.text for token in doc if not token.is_stop] for doc in docs]\n",
    "    return filtered_words\n",
    "data = tweetler.Tweets.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# number of topics\n",
    "num_topics = 5\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "num_topics = lda_model.num_topics\n",
    "num_words_per_topic = 5  # Number of words per topic\n",
    "\n",
    "topics_matrix = lda_model.get_topics()\n",
    "top_words = [[lda_model.id2word[word_id] for word_id, _ in lda_model.get_topic_terms(topic_id, topn=num_words_per_topic)]\n",
    "             for topic_id in range(num_topics)]\n",
    "\n",
    "# Example\n",
    "for topic_id, words in enumerate(top_words):\n",
    "    print(f\"Topic {topic_id + 1}: {', '.join(words)}\")\n",
    "LDA_dict[ID[0]] = top_words\n",
    "download_dictionary_to_pc(LDA_dict,file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Calculating Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculating similarity between your topics and the others.(Might lack further improvements)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def n_similarity(word_set1, word_set2):\n",
    "    tokens1 = nlp(\" \".join(word_set1))\n",
    "    tokens2 = nlp(\" \".join(word_set2))\n",
    "    return tokens1.similarity(tokens2)\n",
    "scores = []\n",
    "similarity_scores = 0\n",
    "for i in range(0,10):\n",
    "    word_set1 = LDA_dict[str(ID[0])][i]\n",
    "    for k in LDA_dict:\n",
    "        for j in range(0,10):\n",
    "            word_set2 = LDA_dict[k][j]\n",
    "            similarity_score = n_similarity(word_set1, word_set2)\n",
    "            similarity_scores += similarity_score\n",
    "        scores.append([f'{k}: ',similarity_scores/10])\n",
    "        similarity_scores = 0\n",
    "scores = pd.DataFrame(scores)\n",
    "scores.columns = [\"name\",\"score\"]\n",
    "scores = pd.DataFrame(scores.groupby(\"name\")[\"score\"].mean())\n",
    "scores = scores.sort_values(by=\"score\",ascending=False)\n",
    "scores = scores.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a Friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here the model matches you with people and asks you which profile do you want to observe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(scores)):\n",
    "    print(f'The {i+1}. Person is : {scores.iloc[i,0]} and his/her relevance_score is: {scores.iloc[i,1]}')\n",
    "chosen = int(input(\"Whose tweets you wish to see? Please enter his/her number: \"))\n",
    "chosen1 = copy.copy(chosen)\n",
    "chosen = scores.iloc[chosen-1,0]\n",
    "chosen = chosen[:-2]\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://twitter.com/i/flow/login\")\n",
    "time.sleep(5)\n",
    "giris = browser.find_element(By.NAME,\"text\")\n",
    "giris.send_keys(ID)\n",
    "giris.send_keys(Keys.RETURN)\n",
    "time.sleep(1.2)\n",
    "browser.find_element(By.NAME,\"password\").send_keys(Password)\n",
    "browser.find_element(By.NAME,\"password\").send_keys(Keys.RETURN)\n",
    "time.sleep(3)\n",
    "browser.find_element(By.XPATH,\"//*[@id='react-root']/div/div/div[2]/main/div/div/div/div[2]/div/div[2]/div/div/div/div[1]/div/div/div/form/div[1]/div/div/div/label/div[2]/div/input\").send_keys(chosen)\n",
    "browser.find_element(By.XPATH,\"//*[@id='react-root']/div/div/div[2]/main/div/div/div/div[2]/div/div[2]/div/div/div/div[1]/div/div/div/form/div[1]/div/div/div/label/div[2]/div/input\").send_keys(Keys.RETURN)\n",
    "time.sleep(3)\n",
    "browser.find_element(By.XPATH,\"//*[@id='react-root']/div/div/div[2]/main/div/div/div/div/div/div[1]/div[1]/div[2]/nav/div/div[2]/div/div[3]/a\").click()\n",
    "time.sleep(4)\n",
    "browser.find_element(By.XPATH,\"//*[@id='react-root']/div/div/div[2]/main/div/div/div/div/div/div[3]/section/div/div/div/div/div[1]/div/div/div/div/div[2]/div[1]/div[1]/div/div[1]/a/div/div[1]/span/span[1]\").click()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
